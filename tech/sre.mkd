# Site Reliability Engineering: How Google Runs Production Systems

## Chapter 1. Introduction

Hope is not a strategy.

size of the team necessarily scales with the load generated by the system.

Traditional operations teams and their counterparts in product development thus often end up in conflict, most visibly over how quickly software can be released to production.

SRE is what happens when you ask a software engineer to design an operations team.

an operations team.

SREs can be broken down into two main categories. 50–60% are Google Software Engineers, or more precisely, people who have been hired via the standard procedure for Google Software Engineers. The other 40–50% are candidates who were very close to the Google Software Engineering qualifications (i.e., 85–99% of the skill set required), and who in addition had a set of technical skills that is useful to SRE but is rare for most software engineers. By far, UNIX system internals and networking (Layer 1 to Layer 3) expertise are the two most common types of alternate technical skills we seek.

team of people who (a) will quickly become bored by performing tasks by hand, and (b) have the skill set necessary to write software to replace their previously manual work,

Google places a 50% cap on the aggregate “ops” work for all SREs — tickets, on-call, manual tasks, etc.

Ensuring a Durable Focus on Engineering

When they are focused on operations work, on average, SREs should receive a maximum of two events per 8–12-hour on-call shift.

enough time to handle the event accurately and quickly, clean up and restore normal service, and then conduct a postmortem.

blame-free postmortem culture, with the goal of exposing faults and applying engineering to fix these faults, rather than avoiding or minimizing them.

Pursuing Maximum Change Velocity Without Violating a Service’s SLO

marginal difference between 99.999% and 100% gets lost in the noise of other unavailability, and the user receives no benefit from the enormous effort required to add that last 0.001% of availability.

product question, which should take the following considerations into account: What level of availability will the users be happy with, given how they use the product? What alternatives are available to users who are dissatisfied with the product’s availability? What happens to users’ usage of the product at different availability levels?

SRE’s goal is no longer “zero outages”; rather, SREs and product developers aim to spend the error budget getting maximum feature velocity.

Monitoring

Monitoring should never require a human to interpret any part of the alerting domain.

three kinds of valid monitoring output:

Alerts Signify that a human needs to take action immediately

Tickets Signify that a human needs to take action, but not immediately.

Logging No one needs to look at this information, but it is recorded for diagnostic or forensic purposes.

Emergency Response

Reliability is a function of mean time to failure (MTTF) and mean time to repair (MTTR) [Sch15]

thinking through and recording the best practices ahead of time in a “playbook” produces roughly a 3x improvement in MTTR as compared to the strategy of “winging it.”

Change Management

roughly 70% of outages are due to changes in a live system. Best practices in this domain use automation

Implementing progressive rollouts Quickly and accurately detecting problems Rolling back changes safely when problems arise

Demand Forecasting and Capacity Planning

steps are mandatory in capacity planning: An accurate organic demand forecast, which extends beyond the lead time required for acquiring capacity An accurate incorporation of inorganic demand sources into the demand forecast Regular load testing of the system to correlate raw capacity (servers, disks, and so on) to service capacity

Provisioning

Provisioning combines both change management and capacity planning.

Efficiency and Performance

paying close attention to the provisioning strategy for a service, and therefore its utilization, provides a very, very big lever on the service’s total costs.

Resource use is a function of demand (load), capacity, and software efficiency.

The End of the Beginning

set of principles, a set of practices, a set of incentives, and a field of endeavor within the larger software engineering discipline.

## Chapter 2. The Production Environment at Google, from the Viewpoint of an SRE

Part II. Principles

looks at SRE through the lens of risk — its assessment, management, and the use of error budgets to provide usefully neutral approaches to service management.

Service level objectives

disentangle indicators from objectives from agreements,

Eliminating toil

monitoring

automation,

release engineering

simplicity is a quality that, once lost, can be extraordinarily difficult to recapture.

## Chapter 3. Embracing Risk

maximizing stability limits how fast new features can be developed

users typically don’t notice the difference between high reliability and extreme reliability

cannot tell the difference between 99.99% and 99.999%

Managing Risk

incremental improvement in reliability may cost 100x more than the previous increment.

The cost of redundant machine/compute resources

The opportunity cost

Measuring Service Risk

focus on unplanned downtime.

Equation 3-1. Time-based availability

instead of using metrics around uptime, we define availability in terms of the request success rate.

Equation 3-2. Aggregate availability

Risk Tolerance of Services

SREs must work with the product owners to turn a set of business goals into explicit objectives to which we can engineer.

Identifying the Risk Tolerance of Consumer Services

Target level of availability

usually depends on the function it provides and how the service is positioned in the marketplace.

We set a lower availability target for YouTube than for our enterprise products because rapid feature development was correspondingly more important.

Types of failures

Which is worse for the service: a constant low rate of failures, or an occasional full-site outage?

Both types of failure may result in the same absolute number of errors, but may have vastly different impacts on the business.

Cost

key factor in determining the appropriate availability target for a service.

consider the following cost/benefit for an example service where each request has equal value: Proposed improvement in availability target: 99.9% → 99.99% Proposed increase in availability: 0.09% Service revenue: $1M Value of improved availability: $1M * 0.0009 = $900

we’ve measured the typical background error rate for ISPs as falling between 0.01% and 1%.

Other service metrics

Examining the risk tolerance of services in relation to metrics besides availability is often fruitful.

AdWords,

key requirement of the system was that the ads should not slow down the search experience.

AdSense,

latency goal for AdSense is to avoid slowing down the rendering of the third-party page when inserting contextual ads.

This means that AdSense ads can generally be served hundreds of milliseconds slower than AdWords ads.

Identifying the Risk Tolerance of Infrastructure Services

by definition, infrastructure components have multiple clients, often with varying needs.

Target level of availability

Such services need low latency and high reliability.

These teams tend to be more concerned about throughput than reliability. Risk tolerance for these two use cases is quite distinct.

One approach to meeting the needs of both use cases is to engineer all infrastructure services to be ultra-reliable.

such an approach is usually far too expensive in practice.

Types of failures

Success for the low-latency user is failure for the user concerned with offline analysis.

Cost

to satisfy these competing constraints in a cost-effective manner is to partition the infrastructure and offer it at multiple independent levels of service.

we can build two types of clusters: low-latency clusters and throughput clusters. The

Example: Frontend infrastructure

consists of reverse proxy and load balancing systems running close to the edge of our network.

Given their critical role, we engineer these systems to deliver an extremely high level of reliability.

consumer services can often limit the visibility of unreliability in backends,

Motivation for Error Budgets

Product development performance is largely evaluated on product velocity,

SRE performance is (unsurprisingly) evaluated based upon reliability of a service,

Information asymmetry between the two teams further amplifies this inherent tension.

following list presents some typical tensions:

Software fault tolerance

Testing

Push frequency

Canary duration and size

Forming Your Error Budget

the two teams jointly define a quarterly error budget based on the service’s service level objective, or SLO

This metric removes the politics from negotiations between the SREs and the product developers

Our practice is then as follows:

Product Management defines an SLO, which sets an expectation of how much uptime the service should have per quarter.

actual uptime is measured by a neutral third party: our monitoring system.

The difference between these two numbers is the “budget” of how much “unreliability” is remaining for the quarter.

As long as the uptime measured is above the SLO — in other words, as long as there is error budget remaining — new releases can be pushed.

Benefits

For example, if product development wants to skimp on testing or increase push velocity and SRE is resistant, the error budget guides the decision. When the budget is large, the product developers can take more risks.

Key Insights

Managing service reliability is largely about managing risk, and managing risk can be costly.

100% is probably never the right reliability target:

An error budget aligns incentives and emphasizes joint ownership between SRE and product development.

## Chapter 4. Service Level Objectives

define service level indicators (SLIs), objectives (SLOs), and agreements (SLAs).

Service Level Terminology

Indicators An SLI

quantitative measure of some aspect of the level of service that is provided.

request latency

error rate,

system throughput,

measurements are often aggregated:

availability,

(Durability

Objectives An SLO

natural structure for SLOs is thus SLI ≤ target, or lower bound ≤ SLI ≤ upper bound.

Agreements Finally, SLAs are service level agreements:

explicit or implicit contract with your users that includes consequences of meeting (or missing) the SLOs they contain.

Whether or not a particular service has an SLA, it’s valuable to define SLIs and SLOs and use them to manage the service.

Indicators in Practice

What Do You and Your Users Care About?

Choosing too many indicators makes it hard to pay the right level of attention

choosing too few may leave significant behaviors of your system unexamined.

Services tend to fall into a few broad categories in terms of the SLIs they find relevant:

User-facing serving systems, such as the Shakespeare search frontends, generally care about availability, latency, and throughput.

Storage systems often emphasize latency, availability, and durability.

Big data systems, such as data processing pipelines, tend to care about throughput and end-to-end latency.

All systems should care about correctness:

Collecting Indicators

Many indicator metrics are most naturally gathered on the server side, using a monitoring system

some systems should be instrumented with client-side collection,

For example, concentrating on the response latency of the Shakespeare search backend might miss poor user latency due to problems with the page’s JavaScript:

Aggregation
